{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdb18e9-e235-4287-8a4e-921932d1bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import soundfile\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "573ba6e1-13c4-40e2-9169-ee467b23bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 (임의 설정)\n",
    "input_data_dir = \"./raw_data/train\"\n",
    "output_data_dir = \"./split_data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c1a24516-d6f7-402f-9c9a-47e3cacd1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_wav_files(directory, output_directory, duration=1.0):\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            preprocess_and_split_wav(file_path, output_directory, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0981a5c8-50af-47c5-9da3-9bdd5cfdb409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 및 분할 (학습을 위해 wav 파일을 1초 단위로 자름)\n",
    "def preprocess_and_split_wav(file_path, output_dir, duration=1.0):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    total_samples = len(y)\n",
    "    num_segments = int(np.ceil(total_samples / sr / duration))\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = int(i * sr * duration)\n",
    "        end = int(min((i + 1) * sr * duration, total_samples))\n",
    "        \n",
    "        segment = y[start:end]\n",
    "        if len(segment) < sr * duration:\n",
    "            padding = np.zeros(int(sr * duration - len(segment)))  \n",
    "            segment = np.concatenate((segment, padding))\n",
    "            \n",
    "        output_file = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(file_path))[0]}_{i}.wav\")\n",
    "        soundfile.write(output_file, segment, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fae25521-82bd-4227-8e14-e85155ca7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 wav 파일에 대해 전처리 수행\n",
    "preprocess_all_wav_files(input_data_dir, output_data_dir, duration=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e00f4bbf-bf31-4cd1-92f5-c4d327956ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_length=128):  \n",
    "        self.samples = []\n",
    "        self.target_classes = {'car_horn': 0, 'dog': 1, 'siren' : 2, 'screaming' : 3, 'talk' : 4}  # 클래스별 정수 레이블 매핑\n",
    "        self.labels = []\n",
    "        self.max_length = max_length\n",
    "\n",
    "        for file_name in os.listdir(data_dir):\n",
    "            if file_name.endswith('.wav'):\n",
    "                self.samples.append(os.path.join(data_dir, file_name))\n",
    "                if \"경적\" in file_name:  \n",
    "                    self.labels.append(self.target_classes['car_horn'])  \n",
    "                elif \"동물\" in file_name: \n",
    "                    self.labels.append(self.target_classes['dog'])\n",
    "                elif \"자동차\" in file_name: \n",
    "                    self.labels.append(self.target_classes['siren'])\n",
    "                elif \"비명\" in file_name: \n",
    "                    self.labels.append(self.target_classes['screaming'])\n",
    "                elif \"대화\" in file_name: \n",
    "                    self.labels.append(self.target_classes['talk'])\n",
    "                    \n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long).to(device) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.samples[idx]\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        if mfccs.shape[1] > self.max_length:\n",
    "            mfccs = mfccs[:, :self.max_length]\n",
    "        else:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, self.max_length - mfccs.shape[1])), mode='constant')\n",
    "\n",
    "        return torch.Tensor(mfccs).to('cuda'), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3d405e17-3dde-4dde-8a20-0cae0088981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SoundClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512) \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        #return F.softmax(x, dim=1)  # softmax를 사용한 결과값 출력\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d199d7bd-5119-4e27-9d83-2b717864309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs = inputs.unsqueeze(1).to(device)  \n",
    "            inputs = F.interpolate(inputs, size=(128, 128))  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), \"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "840a7ea3-0851-47bb-9c30-aa7847c8c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = SoundClassifier(num_classes=5).to(device)\n",
    "\n",
    "#가중치 적용을 위한 각 라벨별 데이터 개수\n",
    "nsumsample = [19050, 19069, 29527, 408, 14727]\n",
    "normedWeights = [1 - (x / sum(nsumsample)) for x in nsumsample]\n",
    "normedWeights = torch.FloatTensor(normedWeights).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(normedWeights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6ef738f3-7772-493a-b877-fa5eecd469e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:38<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:14<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.0919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:04<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:01<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:03<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:06<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:05<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:14<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:04<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [05:02<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0151\n",
      "학습 완료\n"
     ]
    }
   ],
   "source": [
    "dataset = SoundDataset(output_data_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=10)\n",
    "print(\"학습 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7f432109-63b6-485f-8adb-eaf780fb76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "torch.save(model.state_dict(), \"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7559e8de-3716-40f6-96a6-0db81ced7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, criterion):\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            inputs = F.interpolate(inputs, size=(128, 128))\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "\n",
    "    print(f\"Validation Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4ff43a2e-e39a-4499-a36a-4fe6970921a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_dir = \"./split_data/validation\"  \n",
    "validation_dataset = SoundDataset(validation_data_dir) \n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c6a28736-66aa-416e-bf67-99d077a384c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 639/639 [01:20<00:00,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5231, Accuracy: 92.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#모델 검증\n",
    "validate_model(model, validation_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0b6f1839-c05b-41d8-9a9a-22221393732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor(0.5888, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#임의의 데이터 추론 정확도 실험 코드\n",
    "resdataset = SoundDataset(\"./resdataset\")\n",
    "resdataloader = DataLoader(resdataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in resdataloader:\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        inputs = F.interpolate(inputs, size=(128, 128))\n",
    "        labels = labels.to(device)\n",
    "        outputs_probs = resmodel(inputs)\n",
    "        outputs_probs = F.softmax(outputs_probs, dim=1)\n",
    "        predicted_class_idx = torch.argmax(outputs_probs, dim=1).item()\n",
    "        print(predicted_class_idx) #가장 높은 확률의 인덱스\n",
    "        print(max(outputs_probs[0])) #추론한 결과의 확률"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SYS",
   "language": "python",
   "name": "sys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
